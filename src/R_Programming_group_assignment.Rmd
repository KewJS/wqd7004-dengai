---
title: "R_Programming_Project"
output: html_document
date: '2022-06-04'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.duplicate.label = "allow")
```

### Group 2 Member: <br>
**Lee Ming Xiang (S2021030)** <br>
**Kew Jing Sheng (S2021452)** <br>
**Loh Cin Ceat (S2141070)** <br>
**Nurafira Suraya Mazlan (S2105707)** <br>
**Chiow HUi Qin (S2150378)**<br>

## Github link: https://github.com/KewJS/wqd7004-dengai

## Introduction & Objectives
**1.1 Introduction of Dengue Fever**

Dengue fever is a mosquito-borne disease that occurs in tropical and sub-tropical parts of the world. In mild cases, symptoms are similar to the flu: fever, rash, and muscle and joint pain. In severe cases, dengue fever can cause severe bleeding, low blood pressure, and even death.

Because it is carried by mosquitoes, the transmission dynamics of dengue are related to climate variables such as temperature and precipitation. Although the relationship to climate is complex, a growing number of scientists argue that climate change is likely to produce distributional shifts that will have significant public health implications worldwide.

In recent years dengue fever has been spreading. Historically, the disease has been most prevalent in Southeast Asia and the Pacific islands. These days many of the nearly half billion cases per year are occurring in Latin America.

Using environmental data collected by various U.S. Federal Government agencies—from the Centers for Disease Control and Prevention to the National Oceanic and Atmospheric Administration in the U.S. Department of Commerce. In this study, we will predict the number of dengue fever cases reported each week in San Juan, Puerto Rico and Iquitos, Peru, using both regression and classification techniques.

## Loading all libraries needed
```{r }
## Common library used
library (dplyr)
library(ggplot2)

## Library used in Data Preprocessing
library(magrittr)
library(VIM)
library(kableExtra)

## Library used in Exploratory Data Analysis
library(corrplot)
library(RColorBrewer)
library(tidyverse)
library(gridExtra)
library(tidyr)
library(zoo)
library(MASS)
## Library used for both model
library(plyr)
library(readr)
library(caret)
library(e1071)

## Library used for Classification model
library(tidyverse)
library(randomForest)
library(mlbench)
library(RCurl)

## Library used for Regression model
library(caTools)

```

## Data processing (By Lee Ming Xiang)

Both train and test data was imported from the official website. The train data was merged with its label for efficient data processing. Data was studied thoroughly by understanding the data summary, data types, and data structure for both train and test data.The date type of date column was updated from string to date.Data missing value was checked, and K Nearest Neighbor(KNN) imputation was applied, partitioned by each city to fill the null values based on the sample data. The distribution of the data before and after imputation was visualized to validate the imputation results.Box plot of few input features and target features were visualized to study if there is any outlier present in the dataset. There is no outlier observed, and both train and test data were sorted and exported for further processing.

The following code is demonstrating the data pre-processing as described above.


#### Data Importing

```{r}
train <- read.csv('https://raw.githubusercontent.com/KewJS/wqd7004-dengai/main/data/dengue_features_train.csv')
train_label <- read.csv('https://raw.githubusercontent.com/KewJS/wqd7004-dengai/main/data/dengue_labels_train.csv')
test <- read.csv('https://raw.githubusercontent.com/KewJS/wqd7004-dengai/main/data/dengue_features_test.csv')

```

#### Read data (Train)
```{r}
head(train,5) %>%
  kable() %>%
  kable_styling()

```

```{r }
str(train)

```

```{r }
class(train)
```

```{r}
typeof(train)
```

```{r}
names(train)

```

``` {r  }
dim(train) ## Data with 1456 rows,24 columns
```

```{r }
summary(train) %>%  ## 20 years dataset
  kable() %>%
  kable_styling() 
```

### Read data (Train_Label)
```{r }
head(train_label,5)%>%
  kable() %>%
  kable_styling()
```

```{r }
str(train_label)
```

```{r }
dim(train_label) # train label 1456 rows wtih 4 columns
```

```{r }
summary(train_label)%>%
  kable() %>%
  kable_styling()
```

### Read data (Test)
```{r}
head(test,5)%>%
  kable() %>%
  kable_styling()
```

```{r}
str(test)
```

```{r }
dim(test) ## test with 416 rows, 24 columns
```

```{r }
summary(test)%>%
  kable() %>%
  kable_styling()
```

## Data Preprocessing

```{r}

train <- mutate(train, week_start_date= as.Date(week_start_date, format= "%d-%m-%Y"))
test <- mutate(test, week_start_date= as.Date(week_start_date, format= "%Y-%m-%d"))
```


```{r merge train data and their label}
jointtrain <- merge(train, train_label, by = c('city','year','weekofyear'), all.x=TRUE)
colnames(jointtrain)
```

```{r}
summary(jointtrain)%>%
  kable() %>%
  kable_styling() ## data from year 1990 to 2010
```

```{r find unique values for impute prep}
unique(jointtrain$city) ## Data from 2 city
```

```{r subset data for merge data validation}
jointtrain[jointtrain$city == "sj" & jointtrain$year == 1990, ]     
```

```{r check amount if missing value for train data}
sum(is.na(jointtrain)) ## 548 rows out of 1456 rows contain null values
sum(is.na(test)) ## 119 rows out of 416 rows contain null values
```

### Impute Data (Train)

```{r }
sj_jointtrain <- jointtrain[jointtrain$city == "sj", ]    
head(sj_jointtrain,5)
sum(is.na(sj_jointtrain)) ## 40.6% missing values
```

```{r }
iq_jointtrain <- jointtrain[jointtrain$city == "iq", ] 
head(iq_jointtrain,5)%>%
  kable() %>%
  kable_styling()

##Check if the jointrain data is empty
sum(is.na(iq_jointtrain)) ## 32% missing values
```


```{r check column that give null values}
colSums(is.na(sj_jointtrain))%>%
  kable() %>%
  kable_styling()
```

```{r check columns to be imputed}
nums <- unlist(lapply(sj_jointtrain, is.numeric))  
col_num <- colnames(sj_jointtrain[,nums])
options(warn=-1)
imp_col_train <- col_num [col_num != c("year","weekofyear","total_cases")]
head(imp_col_train,5)
```

```{r kNN impute for sj train data}
sj_1 <- kNN(sj_jointtrain,variable = c(imp_col_train))
summary(sj_1)%>%
  kable() %>%
  kable_styling()
```

```{r filter those logical variables}
sj_1 <- subset(sj_1,select=city:total_cases)
head(sj_1,5)
```

```{r check distribution before and after imputation}

##before
#ggplot(sj_jointtrain,aes(x=week_start_date, y=ndvi_ne
#))+geom_line()
# after
#ggplot(sj_1,aes(x=week_start_date, y=ndvi_ne
#))+geom_line()
```

```{r kNN impute for iq train data}
iq_1 <- kNN(iq_jointtrain,variable = c(imp_col_train))
summary(iq_1)%>%
  kable() %>%
  kable_styling()
```

```{r filter those logical variables for iq df}
iq_1 <- subset(iq_1,select=city:total_cases)
head(iq_1,5)
```

```{r }

##before
#ggplot(iq_jointtrain,aes(x=week_start_date, y=ndvi_ne
#))+geom_line()
## after
#ggplot(iq_1,aes(x=week_start_date, y=ndvi_ne
#))+geom_line()
```



```{r }
## clean data
concat_train <- rbind(sj_1, iq_1)
concat_train_sorted <- concat_train[order(concat_train$city, concat_train$week_start_date),]
head(concat_train_sorted,5)
```

```{r }
write.csv(concat_train_sorted, "/Train_cleaned.csv", row.names=FALSE)
```

### Impute Data (Test)

```{r }
options(warn=-1)
nums <- unlist(lapply(sj_jointtrain, is.numeric))  
col_num <- colnames(sj_jointtrain[,nums])
imp_col_test <- col_num[col_num != c('year','weekofyear','total_cases')]
imp_col_test
```

```{r }
sj_test <- test[test$city == "sj", ]    
head(sj_test,5)
sum(is.na(sj_test)) ## 40.6% missing values
```

```{r }
iq_test <- test[test$city == "iq", ]    
head(iq_test)
sum(is.na(iq_test)) ## 40.6% missing values
```

```{r }
sj_test1 <- kNN(sj_test)
summary(sj_test1)%>%
  kable() %>%
  kable_styling()
sj_test1 <- subset(sj_test1,select=city:station_precip_mm)
head(sj_test1,5)
```

```{r}
iq_test1 <- kNN(iq_test)
summary(iq_test1)%>%
  kable() %>%
  kable_styling()
iq_test1 <- subset(iq_test1,select=city:station_precip_mm)
head(iq_test1,5)
```


```{r}
## clean data
concat_test <- rbind(sj_test1, iq_test1)
concat_test_sorted <- concat_test[order(concat_test$city, concat_test$week_start_date),]
head(concat_test_sorted,5)
```

```{r }
write.csv(concat_test_sorted, "/Test_cleaned.csv", row.names=FALSE)
```

## Check Outlier

```{r look summary for concat train}
summary(concat_train_sorted) %>%
  kable() %>%
  kable_styling()
## generally the range of data make sense
## check only column with high range distribution
```

```{r }
# Store the graph
box_plot <- ggplot(concat_train_sorted, aes(y = total_cases))
# Add the geometric object box plot
box_plot +
    geom_boxplot() +coord_flip()+ggtitle("Overall Dengue total cases Boxplot")
```

```{r sj has more cases than iq}
# Store the graph
box_plot <- ggplot(concat_train_sorted, aes(x = city,y = total_cases))
# Add the geometric object box plot
box_plot +
    geom_boxplot() +coord_flip()+ggtitle("Boxplot of Dengue Total Cases by City")
```

```{r}
# Store the graph
box_plot <- ggplot(concat_train_sorted, aes(x = city,y = reanalysis_sat_precip_amt_mm ))
# Add the geometric object box plot
box_plot +
    geom_boxplot() +coord_flip()+ggtitle("Boxplot of reanalysis_sat_precip_amt_mm by City")
```

```{r}
# Store the graph
box_plot <- ggplot(concat_train_sorted, aes(x = city,y = reanalysis_precip_amt_kg_per_m2 ))
# Add the geometric object box plot
box_plot +
    geom_boxplot() +coord_flip()+ggtitle("Boxplot of reanalysis_precip_amt_kg_per_m2  by City")
## seesms outlier for this columns
```


```{r}
# Store the graph
box_plot <- ggplot(concat_train_sorted, aes(x = city,y = station_precip_mm))
# Add the geometric object box plot
box_plot +
    geom_boxplot() +coord_flip()+ggtitle("Boxplot of Dengue station_precip_mm  by City")
```
## Exploratory Data Analysis and Visualization (By Kew Jing Sheng)

### Import Data

```{r}

train_features <- read.csv('https://raw.githubusercontent.com/KewJS/wqd7004-dengai/main/data/dengue_features_train.csv')
train_labels <- read.csv('https://raw.githubusercontent.com/KewJS/wqd7004-dengai/main/data/dengue_labels_train.csv')
test_features <- read.csv('https://raw.githubusercontent.com/KewJS/wqd7004-dengai/main/data/dengue_features_test.csv')

train_cleaned <- read.csv('https://raw.githubusercontent.com/KewJS/wqd7004-dengai/main/data/Train_cleaned.csv')
test_cleaned <- read.csv('https://raw.githubusercontent.com/KewJS/wqd7004-dengai/main/data/Test_cleaned.csv')

```

### Read data (Train)

Read in both before processing data (sj_train_features, iq_train_features) and after processing data (sj_clean_train, iq_clean_train). Outliers using IQR is performed on after processing data to remove outliers in 3 different columns, which are, 'reanalysis_sat_precip_amt_mm', 'reanalysis_precip_amt_kg_per_m2', 'station_precip_mm'
```{r split into sj & iq}
sj_train_features = train_features %>% filter(city == 'sj')
sj_train_features = sj_train_features %>% 
                      select_if(!names(.) %in% c('X'))
sj_train_labels   = train_labels   %>% filter(city == 'sj')
sj_train_labels = sj_train_labels %>% 
                    select_if(!names(.) %in% c('X'))
iq_train_features = train_features %>% filter(city == 'iq')
iq_train_features = iq_train_features %>% 
                      select_if(!names(.) %in% c('X'))
iq_train_labels   = train_labels   %>% filter(city == 'iq')
iq_train_labels = iq_train_labels %>% 
                    select_if(!names(.) %in% c('X'))
sj_clean_train = train_cleaned %>% filter(city == 'sj')
sj_clean_train[['week_start_date']] <- as.POSIXct(sj_clean_train[['week_start_date']],
                                                format = "%Y-%m-%d")
sj_clean_train = sj_clean_train %>% 
  dplyr::filter(reanalysis_sat_precip_amt_mm > quantile(reanalysis_sat_precip_amt_mm, 0.25), 
                reanalysis_sat_precip_amt_mm < quantile(reanalysis_sat_precip_amt_mm, 0.75))
sj_clean_train = sj_clean_train %>% 
  dplyr::filter(reanalysis_precip_amt_kg_per_m2 > quantile(reanalysis_precip_amt_kg_per_m2, 0.25), 
                reanalysis_precip_amt_kg_per_m2 < quantile(reanalysis_precip_amt_kg_per_m2, 0.75))
sj_clean_train = sj_clean_train %>% 
  dplyr::filter(station_precip_mm > quantile(station_precip_mm, 0.25), 
                station_precip_mm < quantile(station_precip_mm, 0.75))
iq_clean_train = train_cleaned %>% filter(city == 'iq')
iq_clean_train[['week_start_date']] <- as.POSIXct(iq_clean_train[['week_start_date']],
                                                  format = "%Y-%m-%d")
```

## Missing Values Visualization

From processing steps, missing values are imputed using KNN model in columns like ndvi_ne, ndvi_nw & ndvi_sw. We can see from the vertical bar plot, after imputation, missing values are imputed. Imputation are performed in 2 different cities, 'sj' and 'iq'.

```{r sj missing values visualization}
pkgs <- c('tidyverse', 'corrplot', 'magrittr', 'zoo', 'RColorBrewer', 'gridExtra','MASS')
invisible(lapply(pkgs, require, character.only = T))

apply(sj_train_features, 2, function(x) 
  round(100 * (length(which(is.na(x))))/length(x) , digits = 1)) %>%
  as.data.frame() %>%
  `names<-`('Percent of Missing Values')
#sj_train_features  %>%
 # summarise_all(list(~is.na(.)))%>%
  #pivot_longer(everything(),
   #            names_to = "variables", values_to="missing") %>%
  #count(variables, missing) %>%
  #ggplot(aes(y=variables,x=n,fill=missing))+
  #geom_col()
apply(sj_clean_train, 2, function(x) 
  round(100 * (length(which(is.na(x))))/length(x) , digits = 1)) %>%
  as.data.frame() %>%
  `names<-`('Percent of Missing Values')
#sj_clean_train  %>%
 # summarise_all(list(~is.na(.)))%>%
  #pivot_longer(everything(),
   #            names_to = "variables", values_to="missing") %>%
  #count(variables, missing) %>%
  #ggplot(aes(y=variables,x=n,fill=missing))+
  #geom_col()
```
## Outliers Visualization

For columns 'reanalysis_sat_precip_amt_mm', 'reanalysis_precip_amt_kg_per_m2' & 'station_precip_mm', we can see the outliers are present but after using IQR-filtering to remove the outliers, we can see that the outliers are removed, as shown in boxplot below. Same as missing values visualization, outliers removal are done in 2 different cities, which are 'sj' & 'iq'.

```{r}
box_plot_after_ol_removed <- ggplot(sj_train_features, aes(y = reanalysis_sat_precip_amt_mm))
box_plot_after_ol_removed +
  geom_boxplot() +coord_flip()+ggtitle("Boxplot of Before Outliers Removal of 'reanalysis_sat_precip_amt_mm'")
box_plot_after_ol_removed <- ggplot(sj_clean_train, aes(y = reanalysis_sat_precip_amt_mm))
box_plot_after_ol_removed +
  geom_boxplot() +coord_flip()+ggtitle("Boxplot of After Outliers Removal of 'reanalysis_sat_precip_amt_mm'")
box_plot_after_ol_removed <- ggplot(sj_train_features, aes(y = reanalysis_precip_amt_kg_per_m2))
box_plot_after_ol_removed +
  geom_boxplot() +coord_flip()+ggtitle("Boxplot of Before Outliers Removal of 'reanalysis_precip_amt_kg_per_m2'")
box_plot_after_ol_removed <- ggplot(sj_clean_train, aes(y = reanalysis_precip_amt_kg_per_m2))
box_plot_after_ol_removed +
  geom_boxplot() +coord_flip()+ggtitle("Boxplot of After Outliers Removal of 'reanalysis_precip_amt_kg_per_m2'")
box_plot_after_ol_removed <- ggplot(sj_train_features, aes(y = station_precip_mm))
box_plot_after_ol_removed +
  geom_boxplot() +coord_flip()+ggtitle("Boxplot of Before Outliers Removal of 'station_precip_mm'")
box_plot_after_ol_removed <- ggplot(sj_clean_train, aes(y = station_precip_mm))
box_plot_after_ol_removed +
  geom_boxplot() +coord_flip()+ggtitle("Boxplot of After Outliers Removal of 'station_precip_mm'")
```

```{r iq missing values visualization}
apply(iq_train_features, 2, function(x) 
  round(100 * (length(which(is.na(x))))/length(x) , digits = 1)) %>%
  as.data.frame() %>%
  `names<-`('Percent of Missing Values')
#iq_train_features  %>%
#  summarise_all(list(~is.na(.)))%>%
#  pivot_longer(everything(),
#               names_to = "variables", values_to="missing") %>%
#  count(variables, missing) %>%
#ggplot(aes(y=variables,x=n,fill=missing))+
#  geom_col()
apply(iq_clean_train, 2, function(x) 
  round(100 * (length(which(is.na(x))))/length(x) , digits = 1)) %>%
  as.data.frame() %>%
  `names<-`('Percent of Missing Values')
#iq_clean_train  %>%
#  summarise_all(list(~is.na(.)))%>%
#  pivot_longer(everything(),
#               names_to = "variables", values_to="missing") %>%
#  count(variables, missing) %>%
#  ggplot(aes(y=variables,x=n,fill=missing))+
#  geom_col()
```
```{r}
box_plot_after_ol_removed <- ggplot(iq_train_features, aes(y = reanalysis_sat_precip_amt_mm))
box_plot_after_ol_removed +
  geom_boxplot() +coord_flip()+ggtitle("Boxplot of Before Outliers Removal of 'reanalysis_sat_precip_amt_mm'")
box_plot_after_ol_removed <- ggplot(iq_clean_train, aes(y = reanalysis_sat_precip_amt_mm))
box_plot_after_ol_removed +
  geom_boxplot() +coord_flip()+ggtitle("Boxplot of After Outliers Removal of 'reanalysis_sat_precip_amt_mm'")
box_plot_after_ol_removed <- ggplot(iq_train_features, aes(y = reanalysis_precip_amt_kg_per_m2))
box_plot_after_ol_removed +
  geom_boxplot() +coord_flip()+ggtitle("Boxplot of Before Outliers Removal of 'reanalysis_precip_amt_kg_per_m2'")
box_plot_after_ol_removed <- ggplot(iq_clean_train, aes(y = reanalysis_precip_amt_kg_per_m2))
box_plot_after_ol_removed +
  geom_boxplot() +coord_flip()+ggtitle("Boxplot of After Outliers Removal of 'reanalysis_precip_amt_kg_per_m2'")
box_plot_after_ol_removed <- ggplot(iq_train_features, aes(y = station_precip_mm))
box_plot_after_ol_removed +
  geom_boxplot() +coord_flip()+ggtitle("Boxplot of Before Outliers Removal of 'station_precip_mm'")
box_plot_after_ol_removed <- ggplot(iq_clean_train, aes(y = station_precip_mm))
box_plot_after_ol_removed +
  geom_boxplot() +coord_flip()+ggtitle("Boxplot of After Outliers Removal of 'station_precip_mm'")
```
## Understanding 'sj' & 'iq' Data Acquired Based on Data Quantity

We can see that 'sj' city has more data than 'iq'.

```{r data shape for each city}
cat('\nSan Juan\n',
    '\t features: ', sj_train_features %>% ncol, 
    '\t entries: ' , sj_train_features %>% nrow,
    '\t labels: '  , sj_train_labels %>% nrow)
cat('\nIquitos\n',
    '\t features: ', iq_train_features %>% ncol, 
    '\t entries: ' , iq_train_features %>% nrow,
    '\t labels: '  , iq_train_labels %>% nrow)
```
## Target Variable: 'total_cases'

Besides from input data quantity difference between 'sj' & 'iq' data, we can see that the data distribution of target variable between 2 cities are also differnt. Hence, modelling will be done based on 2 different cities. 

```{r}
cat('\nSan Juan\n',
    '\t total cases mean: ',      sj_train_labels$total_cases %>% mean(), 
    '\t total cases variance: ' , sj_train_labels$total_cases %>% var() )
cat('\nIquitos\n',
    '\t total cases mean: ',      iq_train_labels$total_cases %>% mean(), 
    '\t total cases variance: ' , iq_train_labels$total_cases %>% var() )
```
## Target Variable: 'total_cases' Period

We can see that 'iq' target variable data ranged from 2000 - 2010, which is contrast to 'sj' where the data is ranged from 1990 to 2010.

```{r}
train_cleaned %>% ggplot() + 
  geom_boxplot(aes(year, total_cases, group=year)) + 
  facet_grid(city ~ ., scale = "free") + 
  ggtitle("Total number of cases per year in each cities")
```
## Target Variable: 'total_cases' Distribution

Although statistical information and period of 'total_cases' in 2 different cities are different, however, the distribution of the data are similar across 2 different cities. Although the distribution size across 2 different cities are similar, the nature of data acquired have different maximum values where we can see 'sj' data has higher range of data across 0 - 500.

```{r}
rbind(iq_train_labels, sj_train_labels) %>% 
  ggplot(aes(x = total_cases,fill = ..count..)) + 
  geom_histogram(bins = 12, colour = 'black') + ggtitle('Total Cases of Dengue') +
  scale_y_continuous(breaks = seq(0,700,100)) + facet_wrap(~city)
```
## Creating Correlation Plot of 'sj' City

We can see that from all the input features, they do not have high correlation, either positive or negative with target variable, 'total_cases', as we see the correlation value does not go over 50%.

```{r sj correlation analysis}
sj_clean_train %>% 
  dplyr::select(-city, -year, -weekofyear, -week_start_date) %>%
  cor(use = 'pairwise.complete.obs') -> M1
corrplot(M1, type="lower", method="color",
           col=brewer.pal(n=8, name="RdBu"), tl.cex=0.7, tl.offset=2)
```
## Creating Correlation Plot of 'iq' City

Same goes to 'iq' city, We can see that from all the input features, they do not have high correlation, either positive or negative with target variable, 'total_cases', as we see the correlation value does not go over 50%.

```{r iq correlation analysis}
iq_clean_train %>% 
  dplyr::select(-city, -year, -weekofyear, -week_start_date) %>%
  cor(use = 'pairwise.complete.obs') -> M2
corrplot(M2, type="lower", method="color",
           col=brewer.pal(n=8, name="RdBu"), tl.cex=0.7, tl.offset=2)
```
## Creating Correlation Barplot of 2 Cities, on 'sj' & 'iq'

Even though the target variable, 'total_cases' in both cities have same distribution, however, they have different highest correlated features. This may due to data statistical different between 2 cities. In 'sj' city, the most positive relation feature with 'total_cases' are 'station_min_temp_c' & 'station_avg_temp_c', while the highest negative correlated feature would be 'ndvi_ne'. In contrast, in 'iq' city, the most positive relation feature with 'total_cases' are 'reanalysis_specific_humidity_g_per_kg' & 'reanalysis_dew_point_temp_k', while the highest negative correlated feature would be 'reanalysis_tdtr_k'. All these highest positive and negative correlated features do not have relationship over 50%.

```{r, out.width="50%"}
sort(M1[21,-21]) %>%  
  as.data.frame %>% 
  `names<-`('correlation') %>%
  ggplot(aes(x = reorder(row.names(.), -correlation), y = correlation, fill = correlation)) + 
  geom_bar(stat='identity', colour = 'black') + scale_fill_continuous(guide = FALSE) + scale_y_continuous(limits =  c(-.15,.25)) +
  labs(title = 'San Jose\n Correlations', x = NULL, y = NULL) + coord_flip() -> cor1
# can use ncol(M1) instead of 21 to generalize the code
sort(M2[21,-21]) %>%  
  as.data.frame %>% 
  `names<-`('correlation') %>%
  ggplot(aes(x = reorder(row.names(.), -correlation), y = correlation, fill = correlation)) + 
  geom_bar(stat='identity', colour = 'black') + scale_fill_continuous(guide = FALSE) + scale_y_continuous(limits =  c(-.15,.25)) +
  labs(title = 'Iquitos\n Correlations', x = NULL, y = NULL) + coord_flip() -> cor2
grid.arrange(cor1, cor2, nrow = 1)
```
## Creating Line Plot to Understand the Highest Correlated Features to 'total_cases' in 'sj'

Now we can see that the data are not correlated well with 'total_cases' across time. This may due to the scale between the data or, the data are not highly correlated. Scaling is suggested for modelling stage later on the input features to match the data distribution.

```{r}
ggplot()+
  geom_line(data=sj_clean_train, mapping=aes(week_start_date, station_min_temp_c), color='blue') + 
  geom_line(data=sj_clean_train, mapping=aes(week_start_date, total_cases), color='red') + 
  ggtitle("Reanalysis_Specific_Humidity_g_per_kg vs Total_Cases over Time")
ggplot()+
  geom_line(data=sj_clean_train, mapping=aes(week_start_date, station_avg_temp_c), color='blue') + 
  geom_line(data=sj_clean_train, mapping=aes(week_start_date, total_cases), color='red') + 
  ggtitle("Reanalysis_Specific_Humidity_g_per_kg vs Total_Cases over Time")
ggplot()+
  geom_line(data=sj_clean_train, mapping=aes(week_start_date, reanalysis_specific_humidity_g_per_kg), color='blue') + 
  geom_line(data=sj_clean_train, mapping=aes(week_start_date, total_cases), color='red') + 
  ggtitle("Reanalysis_Specific_Humidity_g_per_kg vs Total_Cases over Time")
```
## Creating Line Plot to Understand the Highest Correlated Features to 'total_cases' in iq'

Now we can see that the data are not correlated well with 'total_cases' across time. This may due to the scale between the data or, the data are not highly correlated. Scaling is suggested for modelling stage later on the input features to match the data distribution.

```{r}
ggplot()+
  geom_line(data=iq_clean_train, mapping=aes(week_start_date, reanalysis_specific_humidity_g_per_kg), color='blue') + 
  geom_line(data=iq_clean_train, mapping=aes(week_start_date, total_cases), color='red') + 
  ggtitle("Reanalysis_Specific_Humidity_g_per_kg vs Total_Cases over Time")
ggplot()+
  geom_line(data=iq_clean_train, mapping=aes(week_start_date, reanalysis_dew_point_temp_k), color='blue') + 
  geom_line(data=iq_clean_train, mapping=aes(week_start_date, total_cases), color='red') + 
  ggtitle("Reanalysis_Specific_Humidity_g_per_kg vs Total_Cases over Time")
ggplot()+
  geom_line(data=iq_clean_train, mapping=aes(week_start_date, reanalysis_tdtr_k), color='blue') + 
  geom_line(data=iq_clean_train, mapping=aes(week_start_date, total_cases), color='red') + 
  ggtitle("Reanalysis_Specific_Humidity_g_per_kg vs Total_Cases over Time")
```


## Classification Model (By Loh Cin Ceat)

# Classification of Dengue Outbreak

The aims of this section is to predict the occurrence of dengue outbreak by using classification models of support-vector machines (SVM) model with polynomial kernel function and random forest model.According to the WHO, a dengue outbreak is a period of time in which a reported case of a week is more than the sum of the moving average of three 4-week dengue cases plus the value of two standard deviations above the number of dengue cases for the cases four weeks prior. 

## Dengue Outbreak Variable Calculation
The target variable (outbreak) indicates whether there was a dengue outbreak in a particular week-of-the-year in each city.   The dengue outbreak variable (Yes = Dengue Outbreak, No= No dengue outbreak) was created based on the original variable of reported number of dengue cases.To compute the outbreak variable few steps of calculation have to be made. 

1) The average number of dengue cases during the previous four weeks is computed. 

2) The value for the two-standard deviations for four average number of dengue case the previously calculated is determined .

3) The moving average of three 4 week mean dengue cases is then computed.

4) The sum of the moving averages of the three, four-week dengue cases plus the two standard deviations of dengue cases for the previous four weeks is computed .

5) If the weekly instances exceed 2 cases of dengue and  the figure calculated in step 4, an epidemic has most likely occurred.


After calculation, the new dataset is shown as below.

```{r}
df <- read.csv("https://github.com/KewJS/wqd7004-dengai/raw/main/data/Train_cleaned_Outbreak_.csv") 
head(df,5)

```
## Data Preprocessing Before Applying Model

Before applying the data into model for prediction, the data is preprocessed by spliting the deleted unrelated variables and splitting the data according to the city.

1. Delete the Variable used to determine the occurrence of outbreak.
```{r}
df = subset(df, select = -c(`X4wkly.average`,`X2SD.4wkly.Avg`,`Avg.of.3.4wkly.mean`,`Avg.of.3.4wkly.mean.2sd.of.4wkly.mean`) )
```


2. Split the data according to the city
```{r}
iq_df<- df[which(df$city=="iq"),]
sj_df<- df[which(df$city=="sj"),]
```


3. Delete the unrelated variable for prediction for both data of each city.
```{r}
iq_df<-iq_df[-c(1:6),]
iq_df<-subset(iq_df,select = -c(1,2,3,4))
head(iq_df,5)

sj_df<-sj_df[-c(1:6),]
sj_df<-subset(sj_df,select = -c(1,2,3,4))
head(sj_df,5)
```

```{r}
table(iq_df$Outbreak)
table(sj_df$Outbreak)
```

## Simple Random Under Sampling
We found out that there is an obvious  imbalance in the Dengue Outbreak variable. For Iquitos city, there is only 135 cases(26.26%) for Outbreak== "Yes" occurs out of total 514 cases. For San Juan city,there is only 254 cases(27.31%) for Outbreak== "Yes" occurs out of total 930 cases.The imbalance will cause the performance of the classifiers to be biased toward the majority (Outbreak = No) samples and cause low sensitivity. The re sampling will results is reducing amount of data to be trained, which may lower the model's accuracy however, in this scenario, high sensitivity of predicting of true occurrence of Outbreak is crucial. Hence, we performed simple random under sampling to obtain new data with balanced number of "Yes" and "No" in the Outbreak variable.

```{r}

## rows that have "Yes" and "No" entries
iq_Yes <- which(iq_df$Outbreak == "Yes")
iq_No <- which(iq_df$Outbreak == "No")


nsamp <- length(iq_Yes)

set.seed(100)

# a random sample of "No" is picked based on the number of "Yes" in the Outbreak variable.
pick_iq_No<-sample(iq_No, nsamp)
iq_df <- iq_df[c(pick_iq_No,iq_Yes), ]

# Final balanced data
table(iq_df$Outbreak)
```
```{r}

## rows that have "Yes" and "No" entries
sj_Yes <- which(sj_df$Outbreak == "Yes")
sj_No <- which(sj_df$Outbreak == "No")


nsamp <- length(sj_Yes)
set.seed(100)

# a random sample of "No" is picked based on the number of "Yes" in the Outbreak variable.
pick_sj_No<-sample(sj_No, nsamp)
sj_df <- sj_df[c(pick_sj_No,sj_Yes), ]

# Final balanced data
table(sj_df$Outbreak)
```

## Splitting data to train and test split

The data set for each city is split according to 80% training set and 20% testing set to apply for the models for predicting.
```{r}
# To achieve reproducible model; set the random seed number
set.seed(100)

# Performs stratified random split of the data set
iq_TrainingIndex <- createDataPartition(iq_df$Outbreak, p=0.8, list = FALSE)
iq_TrainingSet <- iq_df[iq_TrainingIndex,] # Training Set
iq_TestingSet <- iq_df[-iq_TrainingIndex,] # Test Set

sj_TrainingIndex <- createDataPartition(sj_df$Outbreak, p=0.8, list = FALSE)
sj_TrainingSet <- sj_df[sj_TrainingIndex,] # Training Set
sj_TestingSet <- sj_df[-sj_TrainingIndex,] # Test Set
 
```


## SVM model with polynomial kernel function
support-vector machines (SVM) model with polynomial kernel function with 10 fold cross validation is build to make prediction of dengue outbreak on the training and testing set of both Iquitos and San Juan city.
```{r}
###############################
# SVM model (polynomial kernel)for Iquitos


iq_TrainingSet$Outbreak<- factor(iq_TrainingSet$Outbreak)  ### must do this to prevent error
iq_TestingSet$Outbreak<- factor(iq_TestingSet$Outbreak)


# Build CV model
iq_Model <- train(Outbreak ~ ., data = iq_TrainingSet,
                  method = "svmPoly",
                  na.action = na.omit,
                  preProcess=c("scale","center"),
                  trControl= trainControl(method="cv", number=10),
                  tuneLength = 4
)
summary(iq_Model)


# Apply model for prediction
iq_Model.training <-predict(iq_Model, iq_TrainingSet) # Apply model to make prediction on Training set



iq_Model.testing <-predict(iq_Model, iq_TestingSet) # Apply model to make prediction on Testing set


```

```{r}
###############################
# SVM model (polynomial kernel) for San Juan

# Build Training model
sj_TrainingSet$Outbreak<- factor(sj_TrainingSet$Outbreak)  ### must do this to prevent error
sj_TestingSet$Outbreak<- factor(sj_TestingSet$Outbreak)


# Build SVM model
sj_Model <- train(Outbreak ~ ., data = sj_TrainingSet,
                  method = "svmPoly",
                  na.action = na.omit,
                  preProcess=c("scale","center"),
                  trControl= trainControl(method="cv", number=10),
                  tuneLength = 4
)
summary(sj_Model)


# Apply model for prediction
sj_Model.training <-predict(sj_Model, sj_TrainingSet) # Apply model to make prediction on Training set

sj_Model.testing <-predict(sj_Model, sj_TestingSet) # Apply model to make prediction on Testing set


```

## Random Forest Model 
Random forest classification model with 10 fold cross validation is build to make prediction of dengue outbreak on the training and testing set of both Iquitos and San Juan city.

```{r}
###############################
# Random forest model for Iquitos

# Build CV model
mtry <- sqrt(ncol(iq_TrainingSet))
iq_Model.rf <- train(Outbreak ~ ., data = iq_TrainingSet,
                    method='rf', 
                    metric='Accuracy', 
                    tuneGrid=expand.grid(.mtry=mtry), 
                    trControl=trainControl(method='repeatedcv',number=10,repeats=3)
)

summary(iq_Model.rf)

# Apply model for prediction
iq_Model.training.rf <-predict(iq_Model.rf, iq_TrainingSet) # Apply model to make prediction on Training set
 
iq_Model.testing.rf <-predict(iq_Model.rf, iq_TestingSet) # Apply model to make prediction on Testing set

```

```{r}
###############################
# Random forest model for for San Juan


# Build CV model

mtry <- sqrt(ncol(sj_TrainingSet))
sj_Model.rf <- train(Outbreak ~ ., data = sj_TrainingSet,
                    method='rf', 
                    metric='Accuracy', 
                    tuneGrid=expand.grid(.mtry=mtry), 
                    trControl=trainControl(method='repeatedcv',number=10,repeats=3)
)

summary(sj_Model.rf)%>%
  kable() %>%
  kable_styling()

# Apply model for prediction
sj_Model.training.rf <-predict(sj_Model.rf, sj_TrainingSet) # Apply model to make prediction on Training set

sj_Model.testing.rf <-predict(sj_Model.rf, sj_TestingSet) # Apply model to make prediction on Testing set


```

## Model Evaluation
### Support Vector Machines with Polynomial Kernel 
```{r}
# Model performance for Iquitos (Displays confusion matrix and statistics)
iq_Model.training.confusion <-confusionMatrix(iq_Model.training, iq_TrainingSet$Outbreak,positive = "Yes" )
iq_Model.testing.confusion <-confusionMatrix(iq_Model.testing, iq_TestingSet$Outbreak,positive = "Yes" )

# Training set
iq_Model.train.mat <- iq_Model.training.confusion$table
iq_Model.train.acc <- iq_Model.training.confusion$overall['Accuracy']
iq_Model.train.sens <- iq_Model.training.confusion$byClass['Sensitivity']
iq_Model.train.spec <- iq_Model.training.confusion$byClass['Specificity']

# Testing set
iq_Model.test.mat <- iq_Model.testing.confusion$table
iq_Model.test.acc <- iq_Model.testing.confusion$overall['Accuracy']
iq_Model.test.sens <- iq_Model.testing.confusion$byClass['Sensitivity']
iq_Model.test.spec <- iq_Model.testing.confusion$byClass['Specificity']

# Model performance for San Juan (Displays confusion matrix and statistics)
sj_Model.training.confusion <-confusionMatrix(sj_Model.training, sj_TrainingSet$Outbreak,,positive = "Yes")
sj_Model.testing.confusion <-confusionMatrix(sj_Model.testing, sj_TestingSet$Outbreak,,positive = "Yes")

# Training set
sj_Model.train.mat <- sj_Model.training.confusion$table
sj_Model.train.acc <- sj_Model.training.confusion$overall['Accuracy']
sj_Model.train.sens <- sj_Model.training.confusion$byClass['Sensitivity']
sj_Model.train.spec <- sj_Model.training.confusion$byClass['Specificity']

# Testing set
sj_Model.test.mat <- sj_Model.testing.confusion$table
sj_Model.test.acc <- sj_Model.testing.confusion$overall['Accuracy']
sj_Model.test.sens <- sj_Model.testing.confusion$byClass['Sensitivity']
sj_Model.test.spec <- sj_Model.testing.confusion$byClass['Specificity']

```
## Confusion matrix

### Iquitos
```{r}
# Training set
iq_Model.train.mat
```
The confusion matrix shows 77+69 = 146 correct predictions and 25+17= 42 incorrect ones.

True Positives: 69
True Negatives: 77
False Positives: 25 (Type I error)
False Negatives: 17( Type II error)

```{r}
# Testing set
iq_Model.test.mat
```
The confusion matrix shows 16+14 = 30 correct predictions and 9+7= 16 incorrect ones.

True Positives: 14
True Negatives: 16
False Positives: 9 (Type I error)
False Negatives: 7 ( Type II error)

### san Juan
```{r}
# Training set
sj_Model.train.mat
```
The confusion matrix shows 107+155 = 262 correct predictions and 49+97= 146 incorrect ones.

True Positives: 155
True Negatives: 107
False Positives: 49 (Type I error)
False Negatives: 97 ( Type II error)

```{r}
# Testing set
sj_Model.test.mat
```
The confusion matrix shows 39+29 = 68 correct predictions and 11+21= 32 incorrect ones.

True Positives: 39
True Negatives: 29
False Positives: 11 (Type I error)
False Negatives: 21 ( Type II error)


## Model Evaluation - Statistics

### Iquitos
```{r}
# Training set
iq_Model.train.acc <- iq_Model.training.confusion$overall['Accuracy']
iq_Model.train.sens <- iq_Model.training.confusion$byClass['Sensitivity']
iq_Model.train.spec <- iq_Model.training.confusion$byClass['Specificity']


# Testing set
iq_Model.test.acc <- iq_Model.testing.confusion$overall['Accuracy']
iq_Model.test.sens <- iq_Model.testing.confusion$byClass['Sensitivity']
iq_Model.test.spec <- iq_Model.testing.confusion$byClass['Specificity']

```

### San Juan
```{r}
# Training set
sj_Model.train.acc <- sj_Model.training.confusion$overall['Accuracy']
sj_Model.train.sens <- sj_Model.training.confusion$byClass['Sensitivity']
sj_Model.train.spec <- sj_Model.training.confusion$byClass['Specificity']


#Testing Set
sj_Model.test.acc <- sj_Model.testing.confusion$overall['Accuracy']
sj_Model.test.sens <- sj_Model.testing.confusion$byClass['Sensitivity']
sj_Model.test.spec <- sj_Model.testing.confusion$byClass['Specificity']

```

### Summary of the Statistic of SVM poly Model

```{r}
acc_tbl<-tibble(Model='SVM Poly',
                Set= c("iq_TrainingSet", "iq_TestingSet","sj_TrainingSet", "sj_TestingSet"),                
                Accuracy=c(iq_Model.train.acc,iq_Model.test.acc,sj_Model.train.acc,sj_Model.test.acc),
                Sensitivity = c(iq_Model.train.sens,iq_Model.test.sens,sj_Model.train.sens,sj_Model.test.sens),
                Specificity = c(iq_Model.train.spec,iq_Model.test.spec,sj_Model.train.spec,sj_Model.test.spec))
acc_tbl
```


## Random Forest
```{r}
# Model performance (Displays confusion matrix and statistics)
iq_Model.training.confusion.rf <-confusionMatrix(iq_Model.training.rf, iq_TrainingSet$Outbreak,positive = "Yes")
iq_Model.testing.confusion.rf <-confusionMatrix(iq_Model.testing.rf, iq_TestingSet$Outbreak,positive = "Yes")


# Training set
iq_Model.train.mat.rf <- iq_Model.training.confusion.rf$table
iq_Model.train.acc.rf <- iq_Model.training.confusion.rf$overall['Accuracy']
iq_Model.train.sens.rf <- iq_Model.training.confusion.rf$byClass['Sensitivity']
iq_Model.train.spec.rf <- iq_Model.training.confusion.rf$byClass['Specificity']

# Testing set
iq_Model.test.mat.rf <- iq_Model.testing.confusion.rf$table
iq_Model.test.acc.rf <- iq_Model.testing.confusion.rf$overall['Accuracy']
iq_Model.test.sens.rf <- iq_Model.testing.confusion.rf$byClass['Sensitivity']
iq_Model.test.spec.rf <- iq_Model.testing.confusion.rf$byClass['Specificity']

# Model performance for San Juan (Displays confusion matrix and statistics)
sj_Model.training.confusion.rf <-confusionMatrix(sj_Model.training.rf, sj_TrainingSet$Outbreak,,positive = "Yes")
sj_Model.testing.confusion.rf <-confusionMatrix(sj_Model.testing.rf, sj_TestingSet$Outbreak,,positive = "Yes")

# Training set
sj_Model.train.mat.rf <- sj_Model.training.confusion.rf$table
sj_Model.train.acc.rf <- sj_Model.training.confusion.rf$overall['Accuracy']
sj_Model.train.sens.rf <- sj_Model.training.confusion.rf$byClass['Sensitivity']
sj_Model.train.spec.rf <- sj_Model.training.confusion.rf$byClass['Specificity']

# Testing set
sj_Model.test.mat.rf <- sj_Model.testing.confusion.rf$table
sj_Model.test.acc.rf <- sj_Model.testing.confusion.rf$overall['Accuracy']
sj_Model.test.sens.rf <- sj_Model.testing.confusion.rf$byClass['Sensitivity']
sj_Model.test.spec.rf <- sj_Model.testing.confusion.rf$byClass['Specificity']

```

## Confusion matrix

### Iquitos
```{r}
# Training set
iq_Model.train.mat.rf
```
The confusion matrix shows 94+94 = 188 correct predictions and no incorrect ones.

True Positives: 94
True Negatives: 94
False Positives: 0 (Type I error)
False Negatives: 0 ( Type II error)

```{r}
# Testing set
iq_Model.test.mat.rf
```
The confusion matrix shows 10+20 = 30 correct predictions and 3+13= 16 incorrect ones.

True Positives: 20
True Negatives: 10
False Positives: 3 (Type I error)
False Negatives: 13 ( Type II error)

### san Juan
```{r}
# Training set
sj_Model.train.mat.rf
```
The confusion matrix shows 204+204 = 408 correct predictions and no incorrect ones.

True Positives: 204
True Negatives: 204
False Positives: 0 (Type I error)
False Negatives: 0 ( Type II error)

```{r}
# Testing set
sj_Model.test.mat.rf
```
The confusion matrix shows 35+37 = 72 correct predictions and 13+15= 28 incorrect ones.

True Positives: 37
True Negatives: 35
False Positives: 13 (Type I error)
False Negatives: 15 ( Type II error)


## Model Evaluation - Statistics

### Iquitos
```{r}
# Training set
iq_Model.train.acc.rf <- iq_Model.training.confusion.rf$overall['Accuracy']
iq_Model.train.sens.rf <- iq_Model.training.confusion.rf$byClass['Sensitivity']
iq_Model.train.spec.rf <- iq_Model.training.confusion.rf$byClass['Specificity']


# Testing set
iq_Model.test.acc.rf <- iq_Model.testing.confusion.rf$overall['Accuracy']
iq_Model.test.sens.rf <- iq_Model.testing.confusion.rf$byClass['Sensitivity']
iq_Model.test.spec.rf <- iq_Model.testing.confusion.rf$byClass['Specificity']

```

### san Juan
```{r}
# Training set
sj_Model.train.acc.rf <- sj_Model.training.confusion.rf$overall['Accuracy']
sj_Model.train.sens.rf <- sj_Model.training.confusion.rf$byClass['Sensitivity']
sj_Model.train.spec.rf <- sj_Model.training.confusion.rf$byClass['Specificity']


#Testing Set
sj_Model.test.acc.rf <- sj_Model.testing.confusion.rf$overall['Accuracy']
sj_Model.test.sens.rf <- sj_Model.testing.confusion.rf$byClass['Sensitivity']
sj_Model.test.spec.rf <- sj_Model.testing.confusion.rf$byClass['Specificity']
```

### Summary of the Statistic of SVM poly Model

```{r}
acc_tbl.rf<-tibble(Model='Random Forest',
                Set= c("iq_TrainingSet", "iq_TestingSet","sj_TrainingSet", "sj_TestingSet"),                
                Accuracy=c(iq_Model.train.acc.rf,iq_Model.test.acc.rf,sj_Model.train.acc.rf,sj_Model.test.acc.rf),
                Sensitivity = c(iq_Model.train.sens.rf,iq_Model.test.sens.rf,sj_Model.train.sens.rf,sj_Model.test.sens.rf),
                Specificity = c(iq_Model.train.spec.rf,iq_Model.test.spec.rf,sj_Model.train.spec.rf,sj_Model.test.spec.rf))

summary(acc_tbl.rf)
```

### Summary of Model Evaluation
From the Statistic above we can see that the random forest model overfits since it give a much higher performance score of 100% accuracy on training data and a significant lower performance score on the test data.
  The models were evaluated based on classification accuracy, sensitivity and specificity. Based on the results there is overfitting occurs for Random Forest model. The Random Forest model performed well with 100% accuracy in the training sample for both Iquitos and San Juan city but not for the testing sample. Only the SVM with Polynomial Kernel model performance was consistent for both training and testing samples for both city. 
  Based on testing sample results for Iquitos city, the SVM with Polynomial Kernel model (Accuracy = 65.2%, Sensitivity = 60.8%, Specificity = 69.6%) performed poorer than Random Forest model (Accuracy = 65.2%, Sensitivity =87.0%, Specificity = 43.5%) if compared according to the performance in Sensitivity of the model. 
  For San Juan city, the SVM Linear model (Accuracy = 68.0%, Sensitivity = 78.0%, Specificity = 58.0%) performed slightly better than Random Forest model in sensitivity (Accuracy = 72.0%, Sensitivity =74.0%, Specificity = 70.0%) however poorer in accuracy and specificity.
  As summary, the SVM Linear model is the best prediction model to be used to predict the occurance of dengue outbreak since the model doesn't over or under fit while able to predict with better performance than Random forest model.
  
  
## Regression Mode (By Nurafira Suraya Mazlan)

```{r}
train_reg <-  read.csv("https://raw.githubusercontent.com/KewJS/wqd7004-dengai/main/data/Train_cleaned.csv") 
```

```{r}
head(train_reg,5)
```


```{r split according to city}
iq_regression<- train_reg[which(train_reg$city =="iq"),]
sj_regression<- train_reg[which(train_reg$city =="sj"),]
```

Delete the unrelated variable for prediction for both data of each city.
```{r iq}

iq_regression<-iq_regression[-c(1:6),]
iq_regression<-subset(iq_regression,select = -c(1,2,3,4))
head(iq_regression,5)

```

```{r sj}
sj_regression<-sj_regression[-c(1:6),]
sj_regression<-subset(sj_regression,select = -c(1,2,3,4))
head(sj_regression,5)

```

```{r iq split data}
set.seed(101)
for_splitting <- sample.split (Y = iq_regression$total_cases, SplitRatio = 0.7)
iq_train_reg <- subset(iq_regression, for_splitting == TRUE)
iq_test_reg <- subset(iq_regression, for_splitting == FALSE)
```

```{r iq sanity check}
nrow(iq_train_reg)+nrow(iq_test_reg) == nrow(iq_regression)
```

```{r sj split data}
set.seed(101)
for_splitting <- sample.split (Y = sj_regression$total_cases, SplitRatio = 0.7)
sj_train_reg <- subset(sj_regression, for_splitting == TRUE)
sj_test_reg <- subset(sj_regression, for_splitting == FALSE)
```

```{r sj sanity check}
nrow(sj_train_reg)+nrow(sj_test_reg) == nrow(sj_regression)
```

# IQ regression model

```{r iq linear model}
# Do linear regression model using train data
iq_lm <- lm(total_cases~., data = iq_train_reg)
summary(iq_lm)

# Do prediction using linear regression model 
iq_predict <- predict(iq_lm, newdata = iq_test_reg )
```

```{r}
class(iq_test$total_cases)

```

```{r iq lm performance check}

mse = mean((iq_test_reg$total_cases - iq_predict)^2)
mae = caret::MAE(iq_test_reg$total_cases, iq_predict)
rmse = caret::RMSE(iq_test_reg$total_cases, iq_predict)
SSE <- sum((iq_test_reg$total_cases - iq_predict)**2)
SSR <- sum((iq_predict - mean(iq_test_reg$total_cases)) ** 2)
R2 <- 1 - SSE/(SSE + SSR)

cat("MSE: ", mse, "MAE: ", mae, " RMSE: ", rmse, "R2", R2)

```

```{r iq backward stepwise model}
# Do backward stepwise model
backward_iq <- step (iq_lm, direction = 'backward')
summary (backward_iq)

# Prediction using backward stepwise model
predict_iqbsm <- predict(backward_iq, newdata = iq_test_reg)

```

```{r iq bm performance check}

bm_mse = mean((iq_test_reg$total_cases - predict_iqbsm)^2)
bm_mae = caret::MAE(iq_test_reg$total_cases, predict_iqbsm)
bm_rmse = caret::RMSE(iq_test_reg$total_cases, predict_iqbsm)
bm_SSE <- sum((iq_test_reg$total_cases - predict_iqbsm)**2)
bm_SSR <- sum((predict_iqbsm - mean(iq_test_reg$total_cases)) ** 2)
bm_R2 <- 1 - bm_SSE/(bm_SSE + bm_SSR)

cat("MSE: ", bm_mse, "MAE: ", bm_mae, " RMSE: ", bm_rmse, "R2", bm_R2)
```

```{r iq svr }

# svr model
svr_iq <-svm(formula  = total_cases ~., data = iq_train_reg, type = "eps-regression")
summary(svr_iq)

# predicting using svr model
predict_svr = predict(svr_iq, data = iq_test_reg)
```

```{r iq svr plot}
plot(svr_iq)
plot(iq_test_reg$total_cases,type = 'l',ylab ="Total Cases",pch = 4, col = "blue")
lines(predict_svr,col ="red")
legend("topright", legend=c("Train data", "Predicted data"),
       col=c("blue", "red"), lty=1:2, cex=0.8)
```


```{r svr model performance  check}

svr_mse = mean((iq_test_reg$total_cases - predict_svr)^2)
svr_mae = caret::MAE(iq_test_reg$total_cases, predict_svr)
svr_rmse = caret::RMSE(iq_test_reg$total_cases, predict_svr)
svr_SSE <- sum((iq_test_reg$total_cases - predict_svr)**2)
svr_SSR <- sum((predict_svr - mean(iq_test_reg$total_cases)) ** 2)
svr_R2 <- 1 - svr_SSE/(svr_SSE + svr_SSR)

cat("MSE: ", svr_mse, "MAE: ", svr_mae, " RMSE: ", svr_rmse, "R2", svr_R2)

```

```{r iq comparison vals}

vals <- matrix(c(mse, bm_mse, svr_mse, mae,bm_mae,svr_mae, rmse, bm_rmse,svr_rmse, R2, bm_R2, svr_R2),ncol=3,nrow = 4,byrow=TRUE)
colnames(vals) <- c("Linear regress  ","Backward Stepwise model", "SVR model")
rownames(vals) <- c("MSE","MAE","RMSE","R2")
as.table(vals)
```

# SJ regression model

```{r sj linear model}
# Do linear regression model using train data
sj_lm <- lm(total_cases~., data = sj_train_reg)
summary(sj_lm)

# Do prediction using linear regression model 
sj_predict <- predict(sj_lm, newdata = sj_test_reg )
```

```{r sj lm performance  check}

mse1 = mean((sj_test_reg$total_cases - sj_predict)^2)
mae1 = caret::MAE(sj_test_reg$total_cases, sj_predict)
rmse1 = caret::RMSE(sj_test_reg$total_cases, sj_predict)
SSE1 <- sum((sj_test_reg$total_cases - sj_predict)**2)
SSR1 <- sum((sj_predict - mean(sj_test_reg$total_cases)) ** 2)
R2_1 <- 1 - SSE1/(SSE1 + SSR1)

cat("MSE: ", mse1, "MAE: ", mae1, " RMSE: ", rmse1, "R2", R2_1)

```

```{r sj backward stepwise model }
# Do backward stepwise model
backward_sj <- step (sj_lm, direction = 'backward')

# Prediction using backward stepwise model
predict_sjbsm <- predict(backward_sj, newdata = sj_test_reg)

```

```{r sj bm performance check}

bm_mse1 = mean((sj_test_reg$total_cases - predict_sjbsm)^2)
bm_mae1 = caret::MAE(sj_test_reg$total_cases, predict_sjbsm)
bm_rmse1 = caret::RMSE(sj_test_reg$total_cases, predict_sjbsm)
bm_SSE1 <- sum((sj_test_reg$total_cases - predict_sjbsm)**2)
bm_SSR1 <- sum((predict_sjbsm - mean(sj_test_reg$total_cases)) ** 2)
bm_R2_1 <- 1 - bm_SSE1/(bm_SSE1 + bm_SSR1)

cat("MSE: ", bm_mse1, "MAE: ", bm_mae1, " RMSE: ", bm_rmse1, "R2", bm_R2_1)
```

```{r sj svr }

# svr model
svr_sj <-svm(formula  = total_cases ~., data = sj_train_reg, type = "eps-regression")
summary(svr_sj)

# predicting using svr model
predict_sjsvr = predict(svr_sj, data = sj_test_reg)
```


```{r sj svr plot}
plot(svr_sj)
plot(sj_test_reg$total_cases,type = 'l',ylab ="Total Cases",pch = 4, col = "blue")
lines(predict_sjsvr,col ="red")
legend("topright", legend=c("Train data", "Predicted data"),
       col=c("blue", "red"), lty=1:2, cex=0.8)
```

```{r sj svr model performance check}

svr_mse1 = mean((sj_test_reg$total_cases - predict_sjsvr)^2)
svr_mae1 = caret::MAE(sj_test_reg$total_cases, predict_sjsvr)
svr_rmse1 = caret::RMSE(sj_test_reg$total_cases, predict_sjsvr)
svr_SSE1 <- sum((sj_test_reg$total_cases - predict_sjsvr)**2)
svr_SSR1 <- sum((predict_sjsvr - mean(iq_test_reg$total_cases)) ** 2)
svr_R2_1 <- 1 - svr_SSE1/(svr_SSE1 + svr_SSR)
options(scipen = 0)

cat("MSE: ", svr_mse1, "MAE: ", svr_mae1, " RMSE: ", svr_rmse1, "R2", svr_R2_1)

```

```{r sj comparison vals}

vals1 <- matrix(c(mse1, bm_mse1, svr_mse1, mae1,bm_mae1,svr_mae1, rmse1, bm_rmse1,svr_rmse1, R2_1, bm_R2_1, svr_R2_1),ncol=3,nrow = 4,byrow=TRUE)
options (scipen = 0)
colnames(vals1) <- c("Linear regress  ","Backward Stepwise model", "SVR model")
rownames(vals1) <- c("MSE_sj","MAE_sj","RMSE_sj","R2_sj")
as.table(vals1)
```
The results for three regression model show similar result, however, we will compare based on RMSE and R2 value. 
For RMSE, the lower value, the higher accuracy, meanwhile, for R2, higher value is desirable as it shows the variability in dependent variable. 
Thus, between these three regression models, we choose Linear Model for our study.


## Model Prediction (By Chiow Hui Qin)

In this part of the report, the prediction result of Classification Model and Regression Model will be evaluate. Test_cleaned.csv file that being created during data pre-processing step will be used in both model. 

### Import the test data

```{r}
df_predict <-  read.csv("https://raw.githubusercontent.com/KewJS/wqd7004-dengai/main/data/Test_cleaned.csv") 

#### Read data (Cleaned test data)

head(df_predict,5)%>%
  kable() %>%
  kable_styling()

```

classification model chosen (SVM algorithm is used) : iq_model for Iquitos and sj_model for San Juan
Regression model chosen (Linear regression algorithm is used) : iq_lm for Iquitos and sj_lm for San Juan


### Prepare the prediction data according to city 

Split the data according to the city
```{r}
iq_df<- df_predict[which(df_predict$city=="iq"),]
sj_df<- df_predict[which(df_predict$city=="sj"),]
```


Checking the dataset of Iquitos city
```{r}
head(iq_df,5) %>%
  kable() %>%
  kable_styling()

```

Checking the dataset of San Juan city
```{r}
head(sj_df,5) %>%
  kable() %>%
  kable_styling()

```

### Prediction in Iquitos city

#### Information about the result of the model


Classification model will predict whether an outbreak is happened, the outcome of the prediction will be Yes or No.
Regression model will predict the total cases that was going to happen. 

Below show the test data being used in the prediction of classification model.
```{r}
iq_df$total_cases <- sample(100, size = nrow(iq_df), replace = TRUE)
iq_predict_classification <- predict(iq_Model, iq_df)

head(iq_df[,-1],5)%>%
  kable() %>%
  kable_styling()

```

The Prediction of outbreak for Iquitos city using Classification Model
```{r}
head(iq_predict_classification,5) %>%
  kable() %>%
  kable_styling()

```

```{r}

classification_result =iq_df
classification_result$prediction_ret<- iq_predict_classification

```

```{r}

head(classification_result,5)%>%
  kable() %>%
  kable_styling()

```

## Prediction Analysis for classification model

Seperate the data frame into outbreak happened and outbreak not happen 
```{r}
outbreak_happen_df = classification_result[which(classification_result$prediction_ret=="Yes"),]
outbreak_not_happen_df= classification_result[which(classification_result$prediction_ret=="No"),]
head(outbreak_happen_df,5)%>%
  kable() %>%
  kable_styling()
head(outbreak_not_happen_df,5)%>%
  kable() %>%
  kable_styling()
```

### Relationship between total case and prediction result

#### Classification Analysis of Outbreak going to happen in Iquitos City

```{r}

outbreak_happen_df<-outbreak_happen_df%>%
                     select_if(is.numeric)
head(outbreak_happen_df,5)
sapply(outbreak_happen_df,min)%>%
  kable() %>%
  kable_styling()

```

```{r}

sapply(outbreak_happen_df,max)%>%
  kable() %>%
  kable_styling()
```

In this section, the prediction result dataframe (classification_result) is seperated according to result of outbreak (Yes or No). As observation for an outbreak to happen, the range of respective factors are as shown as below: 

#### Classification analysis of outbreak not happen in Iquitos City

```{r}

outbreak_not_happen_df<-outbreak_not_happen_df%>%
                     select_if(is.numeric)
head(outbreak_not_happen_df,5)
sapply(outbreak_not_happen_df,min)%>%
  kable() %>%
  kable_styling()

```

```{r}

sapply(outbreak_not_happen_df,max)%>%
  kable() %>%
  kable_styling()
```

#### Regression model to predict the total case that will happen in Iquitos City
```{r}
#Regression Model

iq_predict_regression <-predict(iq_lm, iq_df,interval = 'confidence')

head(iq_predict_regression,10)  %>%
  kable() %>%
  kable_styling()
```
Regression model result: 



### Prediction in San Juan City


```{r}
#Classification Model
sj_df$total_cases <- sample(100, size = nrow(sj_df), replace = TRUE)

head(sj_df[,-1],5)%>%
  kable() %>%
  kable_styling()
sj_predict_classification <- predict(sj_Model, sj_df)

```

```{r}
head(sj_predict_classification,5) %>%
  kable() %>%
  kable_styling()
```

```{r}

classification_result_sj =sj_df
classification_result_sj$prediction_ret<- sj_predict_classification

```


```{r}

head(classification_result_sj,5)%>%
  kable() %>%
  kable_styling()
```

## Prediction Analysis for classification model

Seperate the data frame into outbreak happened and outbreak not happen 
```{r}
outbreak_happen_sj_df = classification_result_sj[which(classification_result_sj$prediction_ret=="Yes"),]
outbreak_not_happen_sj_df= classification_result_sj[which(classification_result_sj$prediction_ret=="No"),]

## Data frame for No outbreak of San Juan city
head(outbreak_happen_sj_df,5)%>%
  kable() %>%
  kable_styling()

```

```{r}
#Data frame for out break is happened in San Juan City
head(outbreak_not_happen_sj_df,5)%>%
  kable() %>%
  kable_styling()
```


#### Classification Analysis of outbreak happen in San Juan city
```{r}

outbreak_happen_sj_df<-outbreak_happen_sj_df%>%
                     select_if(is.numeric)

sapply(outbreak_happen_sj_df,min)%>%
  kable() %>%
  kable_styling()

```

```{r}

sapply(outbreak_happen_sj_df,max)%>%
  kable() %>%
  kable_styling()
```

#### Classification Analysis of outbreak not happen in San Juan city

```{r}

outbreak_not_happen_sj_df<-outbreak_not_happen_sj_df%>%
                     select_if(is.numeric)

sapply(outbreak_not_happen_sj_df,min)%>%
  kable() %>%
  kable_styling()

```

```{r}

sapply(outbreak_not_happen_sj_df,max)%>%
  kable() %>%
  kable_styling()
```

####  Regression model to predict the total case that will happen in San Juan City
```{r}
#Regression Model

sj_predict_regression <-predict(sj_lm, sj_df,interval = 'confidence')
head(sj_predict_regression,5)  %>%
  kable() %>%
  kable_styling()

```

## Conclusion

In conclusion, the prediction result of classification model and regression model according to respective city which are Iquitos and San Juan, 
The main reason of this decision is that we believe there is different characteristics exist in different city. for instances, the average air temperature, the humidity and so on which might become the factor that contribute to the factor of dengue. 
Hence, to provide a more accurate prediction result that response better to respective city, both regression model and classification model are train using specific dataset for each city. This implemantation ensure that the result provided is more fit with the city.

In this project, we tend to solve the issues of dengue spreading. Hence, the classification model is aimed to predict whether an outbreak will happen based on the data being recorded previously. Hence, if the model detect that an outbreak is going to happen, it warn the authorities to get prepared for the upcoming outbreak by prepare enough human resources and medicines needed so that there is no bigger loss will be happened. 

Besides that, regression model is used to predict the total cases that might be happened in future. Hence, it acts as the precaution for authorities to take prevention initiatives before the total cases become worse or even reduce the total case that might happened. 

Both model act differently to curb the problem of dengue spreading. They having a different function and there is no comparison can be made to conclude that which model will act better to solve problem. To achieve a better prediction performance, a continuos update of the dataset according to different year and continuos training of the model is important to make sure that the model is grow together with time and data.
